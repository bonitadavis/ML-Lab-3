{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Two: Exploring Image Data\n",
    "\n",
    "***Christopher Cook, Bonita Davis, Anekah Kelley, Davis Lynn***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Business Understanding**\n",
    "\n",
    "This is a dataset of 15,149 songs from 3,083 artists over the course of over a century (1899-2024).  It is sorted by 18 different features:\n",
    "*    Track (string)\n",
    "*    Artist (string)\n",
    "*    Year (int)\n",
    "*    Duration (int, in ms)\n",
    "*    Time Signature (int, numerator of time signature; i.e. 4 means 4/4)\n",
    "*    Danceability (float, 0-10)\n",
    "*    Energy (float 0-1)\n",
    "*    Key (int, 0-11; 0 = C, etc.)\n",
    "*    Loudness (float, loudness in db -47.4=0.92)\n",
    "*    Mode (int, major/minor modality; 0/1)\n",
    "*    Speechiness (float, 0-1)\n",
    "*    Acousticness (float, 0-1)\n",
    "*    Instrumentalness (float 0-1)\n",
    "*    Liveness (float, 0-1)\n",
    "*    Valence (float, 0-1)\n",
    "*    Tempo (tempo, BPM from 0-220)\n",
    "*    Popularity (int, 0-100)\n",
    "*    Genre (string)\n",
    "\n",
    "This data was collected directly from the Spotify API, allowing for additional attributes such as Loudness, Speechiness, etc. The dataset is free to download on Kaggle through the link below.\n",
    "\n",
    "Data Source: https://www.kaggle.com/datasets/thebumpkin/10400-classic-hits-10-genres-1923-to-2023/data\n",
    "\n",
    "The task of this dataset is to serve as a touchpoint for individuals in the music industry to explore trends, compare genres, analyze sonic qualities, and see how classic hits across time periods and genres compare to once another (Kaggle).  The sheer size of the dataset, along with the amount of information provided about each song, makes this an excellent resource to understand musical trends.  It can also be used to see how an individual artist or genre has performed over time, providing insights into how future songs/albums might perform.\n",
    "\n",
    "This is the primary business-case of this dataset and our classifier -- attempting to predict attributes about songs.  This information can be used by companies like Spotify and Apple Music to predict the likelihood that a user will like a song based on the attributes of the songs they have been previously shown to like (if they typically like pop music with a loudness of 97, that information can be used to predict new songs that they may also like).  These businesses care more about the relationships between all of the attributes, as it is the combination of these attributes that creates a listening style.  This information could also be used by a music producer or musician to determine what attributes about songs in different genres/time periods tend to make them popular in order to see how to best leverage the artist's skills to create a hit song.  These businesses care more about the interplay between the attributes that they deem to be relevant based on the artist and popularity, as that gives the best blueprint for success.\n",
    "\n",
    "This model would likely be deployed, although it may differ depending on the user.  Users like Spotify, Apple Music, YouTube Music, and other such companies would use this data constantly, making it a deployed model.  In contrast, while some music producers may consistently run new models, it is possible that it could be semi-offline and just generated once for the relevant artists.  This data was likely collected with these intentions in mind, as classifiers could be trained and sold to such businesses for a sizeable amount of money.\n",
    "\n",
    "The use of such a classifier by companies like Spotify and Apple Music is supported by the fact that Spotify currently uses a similar classifier to create their \"Made For You\" playlist provided to users (https://www.music-tomorrow.com/blog/how-spotify-recommendation-system-works-a-complete-guide-2022).  They use very similar attributes (which is the case because our dataset directly incorporates Spotify information) to predict the likelihood that a user will enjoy a song based on their listening history.  Spotify's algorithm is many times more complex than anything we would be capable of creating at this point in time, so holding ourselves to that standard would be simply foolish.  Additionally, Spotify does not publish an \"accuracy\" rate for predicting songs.  Further complicating the problem, an NIH study showed that roughly 15% of people skip songs that they haven't listened to, before even listening to it, and that others skipped the songs shortly after starting them if they didn't quickly enjoy the song (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7004375/).  While this is a relatively small percentage, it shows the importance of a very accurate classifier.  Ideally, a classifier with a roughly 90% accuracy is likely possible with advanced algorithms, predicts songs with an acceptable margin of error, and will result in benefits for companies.\n",
    "\n",
    "For music producers and artists interested in predicting the success of their songs, a much lower bar of accuracy is acceptable.  Similar classifier algorithms boast an accuracy of anywhere between 55-77% (https://cs229.stanford.edu/proj2015/140_report.pdf, https://towardsdatascience.com/song-popularity-predictor-1ef69735e380, https://www.nature.com/articles/s41598-022-25430-9).  In truth, anything able to predict with an accuracy greater than 50% would be considered successful to some degree, as popularity is highly subjective, changes dramatically over time, and is often a product of purely qualitative features that are difficult to connect to quantitative information, despite best efforts.  Aiming for an accuracy of roughly 65% would put our model squarely in the middle of currently available algorithms, making it both realistic and acceptable in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# find the data type\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassicHit.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# find the data type\n",
    "df = pd.read_csv('ClassicHit.csv')\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataframe information, there are no missing values in the dataset (there are 15150 rows in the data, and 15150 entries in each feature).  Furthermore, the values for the data given are reasonable, as can be seen through the range given in Kaggle.  This range shows the highest and lowest values in the dataset, and all values in these ranges are reasonable, meaning that all values in the dataset are reasonable.\n",
    "\n",
    "If there were missing values in the dataset, we could use mean or median imputation for numerical data (year, duration, danceability, energy, etc.), and mode for the categorical features (genre. etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#data summary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#data summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_des = pd.DataFrame()\n",
    "\n",
    "data_des['Attributes'] = df.columns\n",
    "data_des['Description'] = ['Name of song', 'Name of singer', 'Year released', 'Length of song in milliseconds', 'Musical time signature',\n",
    "        'Suitability of dancing (0-1)', 'Intensity of the song (0-1)', 'Key of the track (0-11)', 'Loudness of track in dB', 'Modality (major or minor)',\n",
    "        'Measures the presence of spoken words (0-1)', 'Measure of how acoustic the track is (0-1)', 'Measure of likelihood that track is instrumental (0-1)',\n",
    "        'Measure of the presence of a live audience (0-1)', 'Measure of musical positiveness (0-1)', 'Speed of the track (BPM)', 'Track popularity (0-100)',\n",
    "        'Musical genre of the song']\n",
    "data_des['Range'] = ['N/A', 'N/A', '1899-2024', '23.0k-3.06m', '3 or 4', '0.0-1.0', '0.0-1.0', '0-11', '-47.4-0.92', '0 or 1', '0.0-1.0', '0.0-1.0', '0.0-1.0', \n",
    "        '0.0-1.0', '0.0-1.0', '0-220', '0-100', 'N/A']\n",
    "\n",
    "data_des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table summarizes the attributes found in this dataset.  As previously stated, there are no missing values in the dataset.\n",
    "\n",
    "We should check whether there are duplicate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "idx = df.duplicated()\n",
    "len(df[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there is one duplicated song. We should see which it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which is a duplicate\n",
    "df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running duplicate code again to double check\n",
    "#check for duplicates\n",
    "idx = df.duplicated()\n",
    "len(df[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a complete duplicate -- \"Happy Now\" by Zedd, released in 2019.  After verifying that it was a perfect duplicate, our group chose to delete the song from our dataset. This completed the cleaning of our data, as we ensured that there was no missing data and there are now no duplicates.\n",
    "\n",
    "The only issue with the data quality is we do not know how many of the features are defined or quantified, such as popularity or loudness. It is unknown whether these features that were provided to us are reliable or consistent. The following features I would consider unknown whether or not they are truly reliable: danceability, energy, loudness, speechiness, instrumentalness, liveness, valence. Another issue is that songs can alternate between key, mode, and time signature. Furthermore only 2 of the 7 modes are listed in dataset (major and minor are present, but not dorian, phrygian, lydian, mixolydian, or locrian). Simply speaking, there are complexities in music lost when only giving one value to key, mode, and time signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE FIRST PARAGRAPH IF WE DON'T END UP USING ERA/ERA2\n",
    "We created a new division in the data called \"era\", which is described as year values < 1982 and year values > 1982. This division helps us visualize how the music changed over time in an easily digestible manner. It also marks the beginning of widespread use of synthesizers in music. We also created a second division in the data, called \"era2\". This variable breaks the music into 6 different time ranges: -1945, 1946-1960, 1961-1975, 1976-1990, 1991-2008, and 2009-. These two variables were extremely useful for data visualization, as it was otherwise nearly impossible to use the year variable.\n",
    "\n",
    "This data has been shown to be ready for our classification model, as it has no duplicates, no missing values, and all values are within a reasonable range.  We are also aware of which values are numeric and non-numeric, an important piece of information as we begin calculations that may require numeric values in order to predict information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm not entirely sure if we need to add a breakdown of variables because I don't think we technically did any pre-processing, but I'll ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Training/Testing Data Split**\n",
    "\n",
    "Our team elected to use the standard 80% training, 20% testing split for our data.  This split is appropriate for several reasons.\n",
    "1.   Our data contains information about over 15,000 songs, making the 80% of training data represent over 12,000 songs.  This is a more than adequate amount of data to train our classifier and provide it with many different combinations of attributes.\n",
    "2.   The size of our dataset also means that the 20% testing data represents over 3,000 songs, making it a large enough chunk of data that we can provide reasonable accuracies that reflect the actual accuracy of the classifier.\n",
    "3.   This split is the the industry standard for datasets, as it has been proven time and time again that a split of roughly 70-80% training and 20-30% testing provides the best resuls (https://scholarworks.utep.edu/cs_techrep/1209/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put division of data here, done in modeling (just move the chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expit\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import plotly\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[1:]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_intercept=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_intercept(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self, X, y):\n",
    "        # Ensure y is numeric, converting categorical to integers or floats\n",
    "        y = y.astype('float')  # or y = y.astype('int') if appropriate\n",
    "    \n",
    "        ydiff = y - self.predict_proba(X, add_intercept=False).ravel()  # Get y difference\n",
    "        ydiff = np.array(ydiff)  # Convert ydiff to a NumPy array to allow multi-dimensional indexing\n",
    "        gradient = np.mean(X * ydiff[:, np.newaxis], axis=0)  # Multiply ydiff as a column vector\n",
    "\n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[:,1:]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return self.w_[:,0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,\n",
    "                                                 self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ClassicHit.csv')\n",
    "\n",
    "#This is how I (Chris) believe the train/test data should be done\n",
    "\n",
    "Xcoloumns = ['Year', 'Duration', 'Time_Signature', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo']\n",
    "\n",
    "X = df[Xcoloumns]\n",
    "y = pd.cut(df['Popularity'], bins=[-float('inf'), 15, 75, float('inf')], labels=[0, 1, 2]) #I am choosing the splits to determine whether a song will be, not populary at all, mediocre, very popular\n",
    "#I chose this way so we can find something meaningful no matter what the classification is for the songs\n",
    "#y = (df['Popularity'] > 75).astype(int) #Testing for binary case\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size=0.2)\n",
    "\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "graph1 = {'x': np.unique(y),\n",
    "          'y': np.bincount(y)/len(y),\n",
    "            'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Class Distribution',\n",
    "                'autosize':False,\n",
    "                'width':400,\n",
    "                'height':400}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.1,500)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
